{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8xszF2gOeQbuuxeUARo6Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saks0106/ML_Frequent-Lookouts/blob/main/Model_03_1LoR_Metrics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jl8XxbceBygz"
      },
      "outputs": [],
      "source": [
        "import os #for smooth flow of execution\n",
        "from sklearn.preprocessing import LabelEncoder  #to encode your target variable\n",
        "from sklearn.model_selection import train_test_split #splitting data\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV #we are using this to seet the pruning parameters\n",
        "from sklearn import tree #picturize/visualize the tree\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix #performence measurement\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OUJYefLDOUdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import tree\n",
        "clf = tree.DecisionTreeClassifier(random_state=0) #random_state =0 ,because i should get optimal/same model everytime\n",
        "clf.fit(x_train,y_train)\n",
        "y_train_pred = clf.predict(x_train)\n",
        "y_test_pred = clf.predict(x_test)\n",
        "\n",
        "plt.figure(figsize=(40,20))\n",
        "features = df.columns #classification attributes\n",
        "classes = ['Not heart disease','heart disease']#target faeture\n",
        "tree.plot_tree(clf,feature_names=features,class_names=classes,filled=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RjPpAUFZCTge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification Matrics"
      ],
      "metadata": {
        "id": "-0Weoj16QNqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function\n",
        "def plot_confusionmatrix(y_train_pred,y_train,dom):\n",
        "    print(f'{dom} Confusion matrix')\n",
        "    cf = confusion_matrix(y_train_pred,y_train)\n",
        "    sns.heatmap(cf,annot=True,yticklabels=classes\n",
        "               ,xticklabels=classes,cmap='Blues', fmt='g')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ciWbJEtEB1tO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Train score {accuracy_score(y_train_pred,y_train)}')\n",
        "print(f'Test score {accuracy_score(y_test_pred,y_test)}')\n",
        "plot_confusionmatrix(y_train_pred,y_train,dom='Train')\n",
        "plot_confusionmatrix(y_test_pred,y_test,dom='Test')"
      ],
      "metadata": {
        "id": "SunhZ9N4B3Rg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4.2 Evaluating a model using the scoring parameter\n",
        "\n",
        "training 5 different versions of training data and evaluate on 5 different versions of test data it takes model, X and y data. cv is cross validation =5 ie 5 different version of model\n",
        "\n",
        "as we split at 0.2. ie 0.8 at test- whatif easy data goes into training data then test can be superfluous. cv tries to avoid those lucky score data as it creates 5 different splits- lucky data is eliminated\n",
        "\n",
        "cross_val_score has evaluated on 5 different train set and gave 5 different results instead of just one in case of clf.score\n",
        "\n",
        "cv can be any no and training data will be split those many times. cv=5 is default\n",
        "\n"
      ],
      "metadata": {
        "id": "Ob0dMrjnOVb8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "#using heart disease classification\n",
        "single_score = clf.score(X_test,y_test)\n",
        "single_score\n",
        "\n",
        "cross_val_score = cross_val_score(clf,X,y,cv=5,scoring=None)# default scoring parameter is mean accuracy\n",
        "cross_val_score\n",
        "\n",
        "{np.mean(cross_val_score)*100:.2f} %\")\n",
        "\n",
        "\n",
        "# cv_acc = cross_val_score(clf,X,y,cv=5, scoring='accuracy')# None -> accuracy\n",
        "# cv_acc\n",
        "# print(f\"The cross validation accuracy is {np.mean(cv_acc)*100:.2f}%\")"
      ],
      "metadata": {
        "id": "jWNzj_2nOcMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#By Hand-Train, Validation, Test will be used ----COMMON FOR ALL 3\n",
        "def evaluate_preds(y_true,y_preds):\n",
        "    \"\"\" \n",
        "    Performs evaluation comparison on y_true lables vs y_preds labels on a classification model\n",
        "    Create a fn if you know, you will call the fn more than once\n",
        "    This fn will compare y_preds to y_true\n",
        "    \"\"\"\n",
        "    accuracy = accuracy_score(y_true,y_preds)\n",
        "    precision = precision_score(y_true,y_preds)\n",
        "    recall = recall_score(y_true,y_preds)\n",
        "    f1 = f1_score(y_true,y_preds)\n",
        "    metric_dict = {\"Accuracy\":round(accuracy,2),\n",
        "                  \"precision\":round(precision,2),\n",
        "                  \"recall\":round(recall,2),\n",
        "                  \"f1\":round(f1,2)}\n",
        "    print(f\"Acc:{accuracy*100:.2f}%\")\n",
        "    print(f\"Precision:{precision*100:.2f}\")\n",
        "    print(f\"Recall:{recall*100:.2f}\")\n",
        "    print(f\"F1:{f1*100:.2f}\")\n",
        "    return metric_dict"
      ],
      "metadata": {
        "id": "3B7CmA0DQzzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using ROC curve\n",
        "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix\n",
        "y_probs = clf.predict_proba(X_test)\n",
        "y_probs[:5], y_test[:5]\n",
        "\n",
        "y_probs_positive = y_probs[:,1] # DON'T DO y_probs[:,1:] as it becomes 2D\n",
        "y_probs_positive[:5]\n",
        "\n",
        "np.array(y_test[:5])\n",
        "\n"
      ],
      "metadata": {
        "id": "TltQmpVDO9KN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to calculate roc_curve, we need fpr,tpr, thresholds in a sequence\n",
        "# we need above 2 for roc_curve\n",
        "\n",
        "fpr,tpr,thresholds = roc_curve(y_test,y_probs_positive)\n",
        "\n",
        "# we need a plotting fn to plot roc_curve\n",
        "def roc_curve(fpr,tpr):\n",
        "    \"\"\"\n",
        "    Plots a ROC curve given  the false positive rate(fpr)\n",
        "    and true positive rate(tpr) of a model. \n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.plot(fpr,tpr,color='blue',label='ROC')\n",
        "    plt.plot([0,1],[0,1], color=\"darkblue\", linestyle=\"--\", label=\"Guessing\")\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.legend()\n",
        "    # Plot line with no predictive power(baseline)\n",
        "    plt.title(\"Receiver Operating Characterisitics (ROC) curve\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "roc_curve(fpr,tpr)\n",
        "\n",
        "roc_auc_score(y_test,y_probs_positive) # auc score tells us that 93% data is covered ie under the curve ie 7-8% data is not giving proper results\n"
      ],
      "metadata": {
        "id": "k7TkySkGPHWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_preds = clf.predict(X_test)\n",
        "confusion_matrix(y_test,y_preds)\n",
        "\n",
        "# #to use plot_confusion_matrix\n",
        "# from sklearn.metrics import plot_confusion_matrix\n",
        "# plot_confusion_matrix(clf,X,y)\n",
        "\n",
        "#without seaborn\n",
        "# from sklearn.metrics import ConfusionMatrixDisplay # 1st method\n",
        "# ConfusionMatrixDisplay.from_estimator(estimator=clf, X=X,y=y) # values are different because we are passing ENTIRE x and y data\n",
        "#ConfusionMatrixDisplay.from_predictions(y_true= y_test, y_pred = y_preds ) # change color by reading documentation"
      ],
      "metadata": {
        "id": "cP8r47ZbPdVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# making a fn to plot confusion_matrix\n",
        "def plot_conf_mat(conf_mat):\n",
        "    fig, ax = plt.subplots(figsize=(3,3))\n",
        "    ax = sns.heatmap(conf_mat, annot=True, cbar=True)\n",
        "    plt.xlabel(\"True label\")\n",
        "    plt.ylabel(\"Predicted label\")\n",
        "    bottom,top = ax.get_ylim()\n",
        "    ax.set_ylim(bottom + 0.5, top-0.5);\n",
        "plot_conf_mat(conf_mat)\n",
        "    "
      ],
      "metadata": {
        "id": "vDk8VoA_PvMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test,y_preds))"
      ],
      "metadata": {
        "id": "GSQTBKPzOfF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regression Matrics"
      ],
      "metadata": {
        "id": "KtV1n7rJQKF1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "y_preds = model.predict(X_test)\n",
        "mean_absolute_error(y_test,y_preds)\n",
        " # it tells us each of our prediction(y_preds) is +-(0.3265...664) y_test. it excludes all -ve values in differences\n",
        "\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "y_preds = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test,y_preds)\n",
        "df['Squared_differences'] = np.square(df['Difference'])"
      ],
      "metadata": {
        "id": "rhfTfpZDQMYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pickle \n",
        "# # save an existing model to file\n",
        "# pickle.dump(gs_clf,open(\"gs_random_forest_model1.pkl\",\"wb\")) # write binaary\n",
        "\n",
        "# #Load  a saved model\n",
        "# loaded_pickel_model = pickle.load(open(\"gs_random_forest_model1.pkl\",\"rb\")) # read binary\n",
        "\n",
        "# # to check it- make some predictions\n",
        "# pickle_y_preds = loaded_pickel_model.predict(X_test)\n",
        "# evaluate_preds(y_test,pickle_y_preds)\n",
        "\n",
        "# gs_y_preds= gs_clf.predict(X_test)\n",
        "# #evalauate the prediction\n",
        "# gs_metrics = evaluate_preds(y_test, gs_y_preds)\n",
        "\n",
        "\n",
        "\n",
        "# # JOblib\n",
        "\n",
        "# from joblib import dump,load \n",
        "# #save model to a file\n",
        "# dump(gs_clf,filename=\"gs_random_forest_model_1.joblib\")\n",
        "\n",
        "# # import the file now\n",
        "# loaded_joblib_model = load(filename=\"gs_random_forest_model_1.joblib\")\n",
        "\n",
        "# # make and evaluate joblib predictions\n",
        "# joblib_y_preds = loaded_joblib_model.predict(X_test)\n",
        "# evaluate_preds(y_test,joblib_y_preds)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kvMdT1ZLQUO0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}